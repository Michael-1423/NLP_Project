{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:15:11.965810Z","iopub.status.busy":"2024-09-24T15:15:11.964722Z","iopub.status.idle":"2024-09-24T15:15:14.883668Z","shell.execute_reply":"2024-09-24T15:15:14.882561Z","shell.execute_reply.started":"2024-09-24T15:15:11.965766Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.login(key = \"\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:15:16.676706Z","iopub.status.busy":"2024-09-24T15:15:16.675834Z","iopub.status.idle":"2024-09-24T15:15:36.441572Z","shell.execute_reply":"2024-09-24T15:15:36.440558Z","shell.execute_reply.started":"2024-09-24T15:15:16.676636Z"},"trusted":true},"outputs":[],"source":["from transformers import MarianTokenizer\n","from datasets import load_dataset\n","from transformers import MarianMTModel\n","from datasets import load_from_disk\n","# Prepare training arguments\n","from transformers import TrainingArguments, Trainer"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:15:46.726915Z","iopub.status.busy":"2024-09-24T15:15:46.726112Z","iopub.status.idle":"2024-09-24T15:15:50.221858Z","shell.execute_reply":"2024-09-24T15:15:50.220700Z","shell.execute_reply.started":"2024-09-24T15:15:46.726872Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3c139349dc549948b17c352f4a720e1","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d51c0ecdbe04abf9da5a9c879bb6d28","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/16.1M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d3fe0a37d2b419bb8cbcbe13c991f97","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/93470 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'translation'],\n","        num_rows: 93470\n","    })\n","})\n","{'id': '0', 'translation': {'en': 'Source: Project GutenbergAudiobook available here', 'es': 'Source: Wikisource & librodot.com'}}\n"]}],"source":["# Load the dataset\n","dataset = load_dataset(\"opus_books\", \"en-es\")\n","\n","\n","print(dataset)\n","print(dataset[\"train\"][0])\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:18:17.945627Z","iopub.status.busy":"2024-09-24T15:18:17.944831Z","iopub.status.idle":"2024-09-24T15:19:35.782596Z","shell.execute_reply":"2024-09-24T15:19:35.781537Z","shell.execute_reply.started":"2024-09-24T15:18:17.945583Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee86d736711f4c11bd2b09833baf8429","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/93470 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-es')\n","tokenizer = MarianTokenizer.from_pretrained('/kaggle/working/path_to_save/tokenizer')\n","\n","# Tokenize the dataset\n","\n","def tokenize_function(examples):\n","    # Extract English ('en') and Spanish ('es') texts\n","    en_texts = [example[\"en\"] for example in examples[\"translation\"]]\n","    es_texts = [example[\"es\"] for example in examples[\"translation\"]]\n","    \n","    # Tokenize the English texts (inputs) and Spanish texts (labels)\n","    inputs = tokenizer(en_texts, padding=\"max_length\", truncation=True)\n","    labels = tokenizer(es_texts, padding=\"max_length\", truncation=True)\n","    \n","    # Return tokenized inputs and corresponding labels\n","    inputs[\"labels\"] = labels[\"input_ids\"]\n","    #print(inputs)\n","    return inputs\n","\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:20:02.011904Z","iopub.status.busy":"2024-09-24T15:20:02.011522Z","iopub.status.idle":"2024-09-24T15:20:02.072600Z","shell.execute_reply":"2024-09-24T15:20:02.071667Z","shell.execute_reply.started":"2024-09-24T15:20:02.011871Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set: 79449 samples\n","Test set: 14021 samples\n"]}],"source":["\n","#print(tokenized_datasets[\"train\"][0])\n","\n","split_tokenized_datasets = tokenized_datasets[\"train\"].train_test_split(test_size=0.15)\n","\n","# Access the tokenized train and test sets\n","tokenized_train = split_tokenized_datasets[\"train\"]\n","tokenized_test = split_tokenized_datasets[\"test\"]\n","\n","# Check the number of samples in each set\n","print(f\"Training set: {len(tokenized_train)} samples\")\n","print(f\"Test set: {len(tokenized_test)} samples\")\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T07:31:27.735994Z","iopub.status.busy":"2024-09-18T07:31:27.735282Z","iopub.status.idle":"2024-09-18T07:31:27.870067Z","shell.execute_reply":"2024-09-18T07:31:27.868977Z","shell.execute_reply.started":"2024-09-18T07:31:27.735949Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('path_to_save/tokenizer/tokenizer_config.json',\n"," 'path_to_save/tokenizer/special_tokens_map.json',\n"," 'path_to_save/tokenizer/vocab.json',\n"," 'path_to_save/tokenizer/source.spm',\n"," 'path_to_save/tokenizer/target.spm',\n"," 'path_to_save/tokenizer/added_tokens.json')"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.save_pretrained(\"path_to_save/tokenizer\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# tokenized_train.save_to_disk(\"path_to_save/tokenized_train\")\n","# tokenized_test.save_to_disk(\"path_to_save/tokenized_test\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:24:11.471918Z","iopub.status.busy":"2024-09-24T15:24:11.470945Z","iopub.status.idle":"2024-09-24T15:24:11.496563Z","shell.execute_reply":"2024-09-24T15:24:11.495719Z","shell.execute_reply.started":"2024-09-24T15:24:11.471871Z"},"trusted":true},"outputs":[],"source":["tokenize_train = load_from_disk(\"/kaggle/input/train-nlp\")\n","tokenize_test = load_from_disk(\"/kaggle/input/test-nlp\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:20:14.524181Z","iopub.status.busy":"2024-09-24T15:20:14.523789Z","iopub.status.idle":"2024-09-24T15:20:14.528846Z","shell.execute_reply":"2024-09-24T15:20:14.527735Z","shell.execute_reply.started":"2024-09-24T15:20:14.524144Z"},"trusted":true},"outputs":[],"source":["# tokenize_test"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T07:31:52.347189Z","iopub.status.busy":"2024-09-18T07:31:52.346759Z","iopub.status.idle":"2024-09-18T07:31:56.662280Z","shell.execute_reply":"2024-09-18T07:31:56.661280Z","shell.execute_reply.started":"2024-09-18T07:31:52.347148Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f4126786ee641b08cc1573a6b3e3bcb","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2034530e0124352816fd6ffdeaf8d3a","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-es')\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T07:32:09.609015Z","iopub.status.busy":"2024-09-18T07:32:09.608297Z","iopub.status.idle":"2024-09-18T07:32:09.614978Z","shell.execute_reply":"2024-09-18T07:32:09.613803Z","shell.execute_reply.started":"2024-09-18T07:32:09.608970Z"},"trusted":true},"outputs":[],"source":["from transformers import TrainerCallback\n","\n","class CustomCallback(TrainerCallback):\n","    def on_save(self, args, state, control, **kwargs):\n","        print(f\"Saving model at step {state.global_step}\")\n","\n","    def on_evaluate(self, args, state, control, **kwargs):\n","        print(f\"Evaluating model at step {state.global_step}\")\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T07:32:15.308385Z","iopub.status.busy":"2024-09-18T07:32:15.307319Z","iopub.status.idle":"2024-09-18T07:32:15.917028Z","shell.execute_reply":"2024-09-18T07:32:15.916166Z","shell.execute_reply.started":"2024-09-18T07:32:15.308338Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}],"source":["\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",          # output directory\n","    evaluation_strategy=\"epoch\",     # evaluate every epoch\n","    save_strategy=\"steps\",           # save checkpoints every specified number of steps\n","    save_steps=500,                  # save every 500 steps\n","    per_device_train_batch_size=8,   # batch size\n","    per_device_eval_batch_size=8,    # batch size for evaluation\n","    num_train_epochs=3,              # number of epochs\n","    save_total_limit=3,              # limit the total number of checkpoints\n",")\n","\n","# Prepare Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenize_train,\n","    eval_dataset=tokenize_test,\n","    callbacks=[CustomCallback]\n",")\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:21:25.838420Z","iopub.status.busy":"2024-09-24T15:21:25.837730Z","iopub.status.idle":"2024-09-24T15:21:25.843027Z","shell.execute_reply":"2024-09-24T15:21:25.841967Z","shell.execute_reply.started":"2024-09-24T15:21:25.838363Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","torch.cuda.empty_cache()\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T07:33:33.087676Z","iopub.status.busy":"2024-09-18T07:33:33.086992Z","iopub.status.idle":"2024-09-18T11:49:52.278523Z","shell.execute_reply":"2024-09-18T11:49:52.277506Z","shell.execute_reply.started":"2024-09-18T07:33:33.087634Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmyselfmichael14\u001b[0m (\u001b[33mmyselfmichael14-na\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.18.1 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.7"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240918_073334-pbhv85ie</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/myselfmichael14-na/huggingface/runs/pbhv85ie' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/myselfmichael14-na/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/myselfmichael14-na/huggingface' target=\"_blank\">https://wandb.ai/myselfmichael14-na/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/myselfmichael14-na/huggingface/runs/pbhv85ie' target=\"_blank\">https://wandb.ai/myselfmichael14-na/huggingface/runs/pbhv85ie</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='14898' max='14898' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [14898/14898 4:15:57, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.166200</td>\n","      <td>0.150085</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.143000</td>\n","      <td>0.139711</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.131600</td>\n","      <td>0.136862</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 1000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 1500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 2000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 2500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 3000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 3500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 4000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 4500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating model at step 4966\n"]},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 5000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 5500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 6000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 6500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 7000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 7500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 8000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 8500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 9000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 9500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating model at step 9932\n"]},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 10000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 10500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 11000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 11500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 12000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 12500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 13000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 13500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 14000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 14500\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]},{"name":"stdout","output_type":"stream","text":["Saving model at step 14898\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating model at step 14898\n"]},{"data":{"text/plain":["TrainOutput(global_step=14898, training_loss=0.15597833686426096, metrics={'train_runtime': 15377.7168, 'train_samples_per_second': 15.5, 'train_steps_per_second': 0.969, 'total_flos': 3.2318294341976064e+16, 'train_loss': 0.15597833686426096, 'epoch': 3.0})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T11:50:06.724152Z","iopub.status.busy":"2024-09-18T11:50:06.723753Z","iopub.status.idle":"2024-09-18T11:56:11.494305Z","shell.execute_reply":"2024-09-18T11:56:11.493296Z","shell.execute_reply.started":"2024-09-18T11:50:06.724112Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='877' max='877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [877/877 06:04]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Evaluating model at step 14898\n"]},{"data":{"text/plain":["{'eval_loss': 0.13686226308345795,\n"," 'eval_runtime': 364.7552,\n"," 'eval_samples_per_second': 38.439,\n"," 'eval_steps_per_second': 2.404,\n"," 'epoch': 3.0}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate()\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T11:56:19.899746Z","iopub.status.busy":"2024-09-18T11:56:19.899339Z","iopub.status.idle":"2024-09-18T11:56:20.615301Z","shell.execute_reply":"2024-09-18T11:56:20.614317Z","shell.execute_reply.started":"2024-09-18T11:56:19.899709Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"]}],"source":["model.save_pretrained(\"path_to_save/model\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:20:54.879009Z","iopub.status.busy":"2024-09-24T15:20:54.878228Z","iopub.status.idle":"2024-09-24T15:20:57.001330Z","shell.execute_reply":"2024-09-24T15:20:57.000456Z","shell.execute_reply.started":"2024-09-24T15:20:54.878967Z"},"trusted":true},"outputs":[],"source":["model = MarianMTModel.from_pretrained('/kaggle/working/path_to_save/model')\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:21:38.386795Z","iopub.status.busy":"2024-09-24T15:21:38.385784Z","iopub.status.idle":"2024-09-24T15:21:39.558511Z","shell.execute_reply":"2024-09-24T15:21:39.557414Z","shell.execute_reply.started":"2024-09-24T15:21:38.386749Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Translated text: ¡Merezca la Navidad!\n"]}],"source":["\n","# Check if a GPU (CUDA) is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Move the model to the selected device (GPU or CPU)\n","model.to(device)\n","\n","# Sample input text (English sentence for translation)\n","input_text = \"Mery Christmas\"\n","\n","# Tokenize the input text and move the tensors to the same device\n","encoded_input = tokenizer(input_text, return_tensors='pt').to(device)\n","\n","# Generate translation output from the model (also on the same device)\n","translation_tokens = model.generate(**encoded_input)\n","\n","# Decode the output token IDs into a readable string\n","translated_text = tokenizer.decode(translation_tokens[0], skip_special_tokens=True)\n","\n","# Display the translated text\n","print(f\"Translated text: {translated_text}\")"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T12:04:57.004667Z","iopub.status.busy":"2024-09-18T12:04:57.004269Z","iopub.status.idle":"2024-09-18T12:05:25.557793Z","shell.execute_reply":"2024-09-18T12:05:25.556635Z","shell.execute_reply.started":"2024-09-18T12:04:57.004628Z"},"trusted":true},"outputs":[],"source":["inputs = [example['translation']['en'] for example in tokenize_test]  # assuming English to Spanish translation\n","references = [[example['translation']['es']] for example in tokenize_test]  # references must be a list of lists"]},{"cell_type":"code","execution_count":23,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-09-18T12:05:54.614954Z","iopub.status.busy":"2024-09-18T12:05:54.614152Z","iopub.status.idle":"2024-09-18T12:30:11.368936Z","shell.execute_reply":"2024-09-18T12:30:11.367407Z","shell.execute_reply.started":"2024-09-18T12:05:54.614914Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m translations \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(t, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[1;32m     19\u001b[0m translated_texts\u001b[38;5;241m.\u001b[39mextend(translations)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2063\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2055\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2056\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2057\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2058\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2059\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2060\u001b[0m     )\n\u001b[1;32m   2062\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2063\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2077\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2078\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2084\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2085\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3334\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3330\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_reorder_cache(\n\u001b[1;32m   3331\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m], beam_idx\n\u001b[1;32m   3332\u001b[0m     )\n\u001b[0;32m-> 3334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate \u001b[38;5;129;01mand\u001b[39;00m output_scores:\n\u001b[1;32m   3335\u001b[0m     beam_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[38;5;241m+\u001b[39m (beam_idx[i],) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(beam_indices))))\n\u001b[1;32m   3337\u001b[0m \u001b[38;5;66;03m# increase cur_len\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from torch.utils.data import DataLoader\n","\n","def collate_fn(batch):\n","    return tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n","\n","# Create a DataLoader for batch processing\n","loader = DataLoader(inputs, batch_size=32, collate_fn=collate_fn)\n","\n","# Translate in batches\n","translated_texts = []\n","model.eval()  # set model to evaluation mode\n","with torch.no_grad():\n","    for batch in loader:\n","        inputs_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        \n","        outputs = model.generate(input_ids=inputs_ids, attention_mask=attention_mask)\n","        translations = [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]\n","        translated_texts.extend(translations)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:22:41.938436Z","iopub.status.busy":"2024-09-24T15:22:41.937985Z","iopub.status.idle":"2024-09-24T15:23:00.209984Z","shell.execute_reply":"2024-09-24T15:23:00.208698Z","shell.execute_reply.started":"2024-09-24T15:22:41.938393Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f3db560dda54aac76426cc7c514589c0cb9d0bb117d5d485c4d73d476cb43647\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}],"source":["!pip install rouge_score"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:23:03.464895Z","iopub.status.busy":"2024-09-24T15:23:03.463834Z","iopub.status.idle":"2024-09-24T15:23:03.963602Z","shell.execute_reply":"2024-09-24T15:23:03.962718Z","shell.execute_reply.started":"2024-09-24T15:23:03.464843Z"},"trusted":true},"outputs":[],"source":["from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu\n","import torch"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:31:25.792008Z","iopub.status.busy":"2024-09-24T15:31:25.791268Z","iopub.status.idle":"2024-09-24T15:31:25.808484Z","shell.execute_reply":"2024-09-24T15:31:25.807349Z","shell.execute_reply.started":"2024-09-24T15:31:25.791966Z"},"trusted":true},"outputs":[],"source":["import torch\n","from nltk.translate.bleu_score import sentence_bleu\n","from rouge_score import rouge_scorer\n","\n","# Check if CUDA (GPU) is available and set device accordingly\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def calculate_perplexity(model, tokenizer, text):\n","    # Tokenize input text and move it to the correct device\n","    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(device)\n","    \n","    # Ensure model is on the same device\n","    model = model.to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model(**inputs, labels=inputs['input_ids'])\n","    \n","    # Calculate perplexity from the loss\n","    loss = outputs.loss\n","    perplexity = torch.exp(loss)\n","    return perplexity.item()\n","\n","def calculate_bleu_score(reference_texts, generated_text):\n","    # Split the reference and generated text into tokens\n","    references = [ref.split() for ref in reference_texts]\n","    candidate = generated_text.split()\n","    \n","    # Calculate BLEU score\n","    score = sentence_bleu(references, candidate)\n","    return score\n","\n","def calculate_rouge_score(reference_text, generated_text):\n","    # Initialize ROUGE scorer\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    \n","    # Calculate ROUGE scores\n","    scores = scorer.score(reference_text, generated_text)\n","    return scores\n","\n","def calculate_f1_score(rouge_scores):\n","    f1_scores = {}\n","    # Calculate F1 score for each ROUGE metric\n","    for key, score in rouge_scores.items():\n","        precision = score.precision\n","        recall = score.recall\n","        if precision + recall > 0:\n","            f1 = 2 * (precision * recall) / (precision + recall)\n","        else:\n","            f1 = 0.0\n","        f1_scores[key + '_f1'] = f1\n","    return f1_scores\n","\n","def evaluate_model(texts, model, tokenizer):\n","    results = []\n","    \n","    # Move the model to the appropriate device\n","    model = model.to(device)\n","    \n","    for text in texts:\n","        # Tokenize input text and move tensors to the correct device\n","        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(device)\n","        \n","        # Generate text using the model\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=512,  \n","                num_beams=5,    \n","                early_stopping=True,\n","                pad_token_id=tokenizer.eos_token_id\n","            )\n","        \n","        # Decode the generated output into text\n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        \n","        # Calculate metrics\n","        perplexity = calculate_perplexity(model, tokenizer, text)\n","        bleu_score = calculate_bleu_score([text], generated_text)\n","        rouge_scores = calculate_rouge_score(text, generated_text)\n","        f1_scores = calculate_f1_score(rouge_scores)\n","        \n","        # Append results\n","        results.append({\n","            'generated_text': generated_text,\n","            'perplexity': perplexity,\n","            'bleu_score': bleu_score,\n","            'rouge_scores': rouge_scores,\n","            'f1_scores': f1_scores\n","        })\n","    \n","    return results\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:24:15.896481Z","iopub.status.busy":"2024-09-24T15:24:15.896080Z","iopub.status.idle":"2024-09-24T15:24:15.903278Z","shell.execute_reply":"2024-09-24T15:24:15.902243Z","shell.execute_reply.started":"2024-09-24T15:24:15.896447Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['id', 'translation', 'input_ids', 'attention_mask', 'labels'],\n","    num_rows: 14021\n","})"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["tokenize_test"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T15:31:28.902594Z","iopub.status.busy":"2024-09-24T15:31:28.901646Z","iopub.status.idle":"2024-09-24T17:29:29.378405Z","shell.execute_reply":"2024-09-24T17:29:29.377527Z","shell.execute_reply.started":"2024-09-24T15:31:28.902550Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 3-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"]}],"source":["import torch\n","\n","# Check if CUDA (GPU) is available and set device accordingly\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def evaluate_model_on_dataset(dataset, model, tokenizer):\n","    results = []\n","    \n","    # Move model to the same device\n","    model = model.to(device)\n","    \n","    for example in dataset:\n","        # Decode input_ids and labels (assuming 'labels' are reference translations)\n","        input_text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n","        reference_text = tokenizer.decode(example['labels'], skip_special_tokens=True)\n","        \n","        # Tokenize input text\n","        inputs = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512).to(device)\n","\n","        # Evaluate the model on the input text\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=512,  \n","                num_beams=5,    \n","                early_stopping=True,\n","                pad_token_id=tokenizer.eos_token_id\n","            )\n","        \n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        \n","        # Calculate metrics\n","        perplexity = calculate_perplexity(model, tokenizer, input_text)\n","        bleu_score = calculate_bleu_score([reference_text], generated_text)\n","        rouge_scores = calculate_rouge_score(reference_text, generated_text)\n","        f1_scores = calculate_f1_score(rouge_scores)\n","        \n","        # Add the results for each example\n","        results.append({\n","            'id': example['id'],\n","            'input_text': input_text,\n","            'reference_text': reference_text,\n","            'generated_text': generated_text,\n","            'perplexity': perplexity,\n","            'bleu_score': bleu_score,\n","            'rouge_scores': rouge_scores,\n","            'f1_scores': f1_scores\n","        })\n","    \n","    return results\n","\n","# Example usage\n","results = evaluate_model_on_dataset(tokenize_test, model, tokenizer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5717802,"sourceId":9415044,"sourceType":"datasetVersion"},{"datasetId":5717813,"sourceId":9415064,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
